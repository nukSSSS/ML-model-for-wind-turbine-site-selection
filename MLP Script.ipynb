{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52386ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer perceptrons model (MLP) with best parameters\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n",
    "                             confusion_matrix, classification_report, roc_curve, \n",
    "                             roc_auc_score, log_loss, matthews_corrcoef, cohen_kappa_score, \n",
    "                             balanced_accuracy_score)\n",
    "\n",
    "# Load the combined dataset\n",
    "\n",
    "'''Here and below  data is a combination of wind turbines with random points from suitable area '''\n",
    "\n",
    "data = pd.read_csv('The path to the data', \n",
    "                   sep=';', decimal=',')\n",
    "# Replace NaN values if found\n",
    "data.fillna(0, inplace=True)\n",
    "\n",
    "# Save the x and y columns\n",
    "columns_to_remove = ['X', 'Y']\n",
    "removed_columns = data[columns_to_remove]\n",
    "\n",
    "# Remove the x and y columns from the DataFrame\n",
    "data = data.drop(columns=columns_to_remove)\n",
    "\n",
    "# Randomly shuffle the dataset\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Separate features and labels\n",
    "X = data.drop(columns=['suitability'])  # 'suitability' is the target column\n",
    "y = data['suitability']\n",
    "\n",
    "# Split the data into training, validation, and test sets (e.g., 70% train, 15% val, 15% test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define and train the MLPClassifier\n",
    "# mlp = MLPClassifier(hidden_layer_sizes=(128, 128), \n",
    "#                     activation='tanh', \n",
    "#                     solver='adam', \n",
    "#                     alpha=0.01,\n",
    "#                     max_iter=200, \n",
    "#                     random_state=42, \n",
    "#                     early_stopping=True, \n",
    "#                     validation_fraction=0.1)\n",
    "\n",
    "# the MLPClassifier with best parameters\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(64, 128, 64), \n",
    "                    activation='tanh', \n",
    "                    solver='adam', \n",
    "                    alpha=0.0001,\n",
    "                    batch_size=128,\n",
    "                    max_iter=200, \n",
    "                    random_state=42, \n",
    "                    early_stopping=True, \n",
    "                    learning_rate='constant', \n",
    "                    learning_rate_init=0.01)\n",
    "\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict probabilities for new points (without labels)\n",
    "df_new_points = pd.read_csv('The path to the data', \n",
    "                            sep=';', decimal=',')\n",
    "# Replace NaN values if found\n",
    "df_new_points.fillna(0, inplace=True)\n",
    "\n",
    "# Save the x and y columns for new data\n",
    "removed_columns_new = df_new_points[columns_to_remove]\n",
    "df_new_points = df_new_points.drop(columns=columns_to_remove)\n",
    "\n",
    "X_new_points = scaler.transform(df_new_points)\n",
    "y_new_pred_probs = mlp.predict_proba(X_new_points)[:, 1]  # Probability of being suitable\n",
    "\n",
    "# Add predictions to the new points DataFrame\n",
    "df_new_points[\"suitability_score\"] = y_new_pred_probs\n",
    "\n",
    "# Restore the x and y columns in the final DataFrame\n",
    "df_new_points[columns_to_remove] = removed_columns_new\n",
    "\n",
    "# Save results to a CSV\n",
    "df_new_points.to_csv(\"The path to predicted_scores.csv\",\n",
    "                     sep=';', decimal=',', index=False)\n",
    "\n",
    "# Save results to an Excel\n",
    "df_new_points.to_excel(\"The path to predicted_scores.xlsx\")\n",
    "\n",
    "print(\"Predictions saved to 'predicted_suitability_scores.csv/xlsx'\")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nValidation Set Performance:\")\n",
    "y_val_pred = mlp.predict(X_val_scaled)\n",
    "print(confusion_matrix(y_val, y_val_pred))\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "y_test_pred = mlp.predict(X_test_scaled)\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Compute metrics\n",
    "y_pred_test = mlp.predict(X_test_scaled)\n",
    "y_pred_proba_test = mlp.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_test)\n",
    "precision = precision_score(y_test, y_pred_test)\n",
    "recall = recall_score(y_test, y_pred_test)\n",
    "f1 = f1_score(y_test, y_pred_test)\n",
    "mcc = matthews_corrcoef(y_test, y_pred_test)\n",
    "cohen_kappa = cohen_kappa_score(y_test, y_pred_test)\n",
    "balanced_accuracy = balanced_accuracy_score(y_test, y_pred_test)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba_test)\n",
    "\n",
    "# Print metrics\n",
    "print(\"Test Set Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n",
    "print(f\"Cohen's Kappa: {cohen_kappa:.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy:.4f}\")\n",
    "print(f\"Log Loss: {log_loss_value:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba_test)\n",
    "auc = roc_auc_score(y_test, y_pred_proba_test)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"AUC-ROC Score: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48679ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting zero from the dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Parameters\n",
    "input_file = '/mlp/inputXY.csv'  # Input file\n",
    "output_file = '/mlp/inputXY_no0.csv'  # Output file\n",
    "columns = ['Wind_speed_100m', 'Power_density_100m', 'Natura 2000 network (habitats)',\n",
    "           'Natura 2000 network (birds)', 'National parks', 'Landscape parks', 'Reserves',\n",
    "           'Natural landscape complexes', 'Ecological sites', 'Documentation posts', 'Protected landscape areas',\n",
    "           'Monuments of nature points', 'Monuments of nature polygons', 'adms', 'bubd', 'kusc', 'kuza', 'oimk',\n",
    "           'ptlz', 'ptut', 'ptwp', 'ptwz', 'skdr', 'suln', 'swkn', 'swrm', 'swrs'\n",
    "          ]  # List of columns\n",
    "value_to_remove = 0  # Value to remove\n",
    "\n",
    "# Read the file\n",
    "df = pd.read_csv(input_file, sep=';', decimal=',')\n",
    "\n",
    "# column_names = df.columns.tolist()\n",
    "# print(\"List of column names:\", column_names)\n",
    "\n",
    "# Number of rows before filtering\n",
    "initial_row_count = len(df)\n",
    "\n",
    "# Filtering rows\n",
    "for column in columns:\n",
    "    df = df[df[column] != value_to_remove]\n",
    "\n",
    "# Number of rows after filtering\n",
    "final_row_count = len(df)\n",
    "\n",
    "# Calculate the number of removed rows\n",
    "removed_rows = initial_row_count - final_row_count\n",
    "\n",
    "# Save the result\n",
    "df.to_csv(output_file, sep=';', decimal=',', index=False)\n",
    "\n",
    "print(f\"{removed_rows} row(s) were removed. The file has been saved as '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce48584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting zero from the new points dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Parameters\n",
    "input_file = '/RP_100k_PL_XY.csv'  # Input file\n",
    "output_file = '/RP_100k_PL_XY_no0.csv'  # Output file\n",
    "columns = ['Wind_speed_100m', 'Power_density_100m', 'Natura 2000 network (habitats)',\n",
    "           'Natura 2000 network (birds)', 'National parks', 'Landscape parks', 'Reserves',\n",
    "           'Natural landscape complexes', 'Ecological sites', 'Documentation posts', 'Protected landscape areas',\n",
    "           'Monuments of nature points', 'Monuments of nature polygons', 'adms', 'bubd', 'kusc', 'kuza', 'oimk',\n",
    "           'ptlz', 'ptut', 'ptwp', 'ptwz', 'skdr', 'suln', 'swkn', 'swrm', 'swrs'\n",
    "          ]  # List of columns\n",
    "value_to_remove = 0  # Value to remove\n",
    "\n",
    "# Read the file\n",
    "df = pd.read_csv(input_file, sep=';', decimal='.')\n",
    "\n",
    "# column_names = df.columns.tolist()\n",
    "# print(\"List of column names:\", column_names)\n",
    "\n",
    "# Number of rows before filtering\n",
    "initial_row_count = len(df)\n",
    "\n",
    "# Filtering rows\n",
    "for column in columns:\n",
    "    df = df[df[column] != value_to_remove]\n",
    "\n",
    "# Number of rows after filtering\n",
    "final_row_count = len(df)\n",
    "\n",
    "# Calculate the number of removed rows\n",
    "removed_rows = initial_row_count - final_row_count\n",
    "\n",
    "# Save the result\n",
    "df.to_csv(output_file, sep=';', decimal=',', index=False)\n",
    "\n",
    "print(f\"{removed_rows} row(s) were removed. The file has been saved as '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b3e46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Multilayer perceptrons model (MLP) with best model parameters. No zero in input data '''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n",
    "                             confusion_matrix, classification_report, roc_curve, \n",
    "                             roc_auc_score, log_loss, matthews_corrcoef, cohen_kappa_score, \n",
    "                             balanced_accuracy_score)\n",
    "\n",
    "# Load the combined dataset\n",
    "data = pd.read_csv('The path to the data', \n",
    "                   sep=';', decimal=',')\n",
    "# Replace NaN values if found\n",
    "data.fillna(0, inplace=True)\n",
    "\n",
    "# Save the x and y columns\n",
    "columns_to_remove = ['X', 'Y']\n",
    "removed_columns = data[columns_to_remove]\n",
    "\n",
    "# Remove the x and y columns from the DataFrame\n",
    "data = data.drop(columns=columns_to_remove)\n",
    "\n",
    "# Randomly shuffle the dataset\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Separate features and labels\n",
    "X = data.drop(columns=['suitability'])  # 'suitability' is the target column\n",
    "y = data['suitability']\n",
    "\n",
    "# Split the data into training, validation, and test sets (e.g., 70% train, 15% val, 15% test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define and train the MLPClassifier\n",
    "# mlp = MLPClassifier(hidden_layer_sizes=(128, 128), \n",
    "#                     activation='tanh', \n",
    "#                     solver='adam', \n",
    "#                     alpha=0.01,\n",
    "#                     max_iter=200, \n",
    "#                     random_state=42, \n",
    "#                     early_stopping=True, \n",
    "#                     validation_fraction=0.1)\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(64, 128, 64), \n",
    "                    activation='tanh', \n",
    "                    solver='adam', \n",
    "                    alpha=0.0001,\n",
    "                    batch_size=128,\n",
    "                    max_iter=200, \n",
    "                    random_state=42, \n",
    "                    early_stopping=True, \n",
    "                    learning_rate='constant', \n",
    "                    learning_rate_init=0.01)\n",
    "\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict probabilities for new points (without labels)\n",
    "df_new_points = pd.read_csv('The path to /RP_100k_PL_XY_no0.csv', \n",
    "                            sep=';', decimal=',')\n",
    "# Replace NaN values if found\n",
    "df_new_points.fillna(0, inplace=True)\n",
    "\n",
    "# Save the x and y columns for new data\n",
    "removed_columns_new = df_new_points[columns_to_remove]\n",
    "df_new_points = df_new_points.drop(columns=columns_to_remove)\n",
    "\n",
    "X_new_points = scaler.transform(df_new_points)\n",
    "y_new_pred_probs = mlp.predict_proba(X_new_points)[:, 1]  # Probability of being suitable\n",
    "\n",
    "# Add predictions to the new points DataFrame\n",
    "df_new_points[\"suitability_score\"] = y_new_pred_probs\n",
    "\n",
    "# Restore the x and y columns in the final DataFrame\n",
    "df_new_points[columns_to_remove] = removed_columns_new\n",
    "\n",
    "# Save results to a CSV\n",
    "df_new_points.to_csv(\"/predicted_scores_no0.csv\",\n",
    "                     sep=';', decimal=',', index=False)\n",
    "\n",
    "# Save results to an Excel\n",
    "df_new_points.to_excel(\"/predicted_scores_no0.xlsx\")\n",
    "\n",
    "print(\"Predictions saved to 'predicted_suitability_scores_no0.csv/xlsx'\")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nValidation Set Performance:\")\n",
    "y_val_pred = mlp.predict(X_val_scaled)\n",
    "print(confusion_matrix(y_val, y_val_pred))\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "y_test_pred = mlp.predict(X_test_scaled)\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Compute metrics\n",
    "y_pred_test = mlp.predict(X_test_scaled)\n",
    "y_pred_proba_test = mlp.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_test)\n",
    "precision = precision_score(y_test, y_pred_test)\n",
    "recall = recall_score(y_test, y_pred_test)\n",
    "f1 = f1_score(y_test, y_pred_test)\n",
    "mcc = matthews_corrcoef(y_test, y_pred_test)\n",
    "cohen_kappa = cohen_kappa_score(y_test, y_pred_test)\n",
    "balanced_accuracy = balanced_accuracy_score(y_test, y_pred_test)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba_test)\n",
    "\n",
    "# Print metrics\n",
    "print(\"Test Set Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n",
    "print(f\"Cohen's Kappa: {cohen_kappa:.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy:.4f}\")\n",
    "print(f\"Log Loss: {log_loss_value:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show() \n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba_test)\n",
    "auc = roc_auc_score(y_test, y_pred_proba_test)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"AUC-ROC Score: {auc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
